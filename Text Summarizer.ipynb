{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import indian\n",
    "from nltk.tag import tnt \n",
    "\n",
    "with open(\"/home/akanksha/HindiWN_1_5/database/data_txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    wordNetFile = file.read()\n",
    "\n",
    "wordNetFile = wordNetFile.strip().splitlines()\n",
    "wordDictionary = []\n",
    "for sent in wordNetFile:\n",
    "    wordDictionary.append(sent.split(\" \")[3].split(\":\"))\n",
    "synonym_dictionary = {}\n",
    "for i in wordDictionary:\n",
    "    for j in i:\n",
    "        if j not in synonym_dictionary:\n",
    "            synonym_dictionary[j] = set()\n",
    "        for k in i:\n",
    "                synonym_dictionary[j].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_file=\"4.txt\"\n",
    "percentage=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from string import punctuation\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from wxconv import WXC\n",
    "import subprocess \n",
    "import os.path\n",
    "import json\n",
    "import copy\n",
    "import nltk\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "\n",
    "with open(\"support_data/stopwords.txt\", 'r', encoding = \"utf-8\") as stop:\n",
    "    stopWords = stop.read().splitlines()\n",
    "    \n",
    "    \n",
    "with open(\"/home/akanksha/Text_Summarizer/amarujala/\"+inp_file,\"r\") as g:\n",
    "    input_text = g.read().strip()\n",
    "    \n",
    "with open(\"support_data/month_day.txt\", 'r', encoding = \"utf-8\") as mon:\n",
    "    month_day = mon.read().splitlines()\n",
    "     \n",
    "    \n",
    "def remove_stopwords(text):\n",
    "    text=[i for i in text if i not in stopWords]\n",
    "    return(text)\n",
    "\n",
    "\n",
    "def generateBigrams(tokenized_list):\n",
    "    bigramsDict = dict()\n",
    "    bigramsWordsList = list()\n",
    "    bigram = list()\n",
    "    bigrams = list(nltk.bigrams(list(reduce(lambda x,y: x+y,tokenized_list))))\n",
    "    cfd = nltk.ConditionalFreqDist(bigrams)\n",
    "    for inneritems in cfd.items():\n",
    "        for items in inneritems[1].items():\n",
    "            if items[1] > 2:\n",
    "                if items[1] not in bigramsDict:\n",
    "                    bigramsDict[items[1]] = []\n",
    "                    bigramsDict[items[1]].append([inneritems[0], items[0]])\n",
    "                else:\n",
    "                    bigramsDict[items[1]].append([inneritems[0], items[0]])\n",
    "                bigramsWordsList.append([inneritems[0], items[0]])\n",
    "    return(bigramsDict , bigramsWordsList, bigram)\n",
    "\n",
    "\n",
    "def countBigrams(sentences): \n",
    "    sentenceBigrams = list(nltk.bigrams(sentences))\n",
    "    count = 0\n",
    "    for items in sentenceBigrams:\n",
    "        if list(items) in bigramsWordsList:\n",
    "            count +=  1\n",
    "    return count/len(sent)\n",
    "\n",
    "\n",
    "def partsOfSpeechTagger(localWordList):\n",
    "    train_data = indian.tagged_sents('hindi.pos') \n",
    "    tnt_pos_tagger = tnt.TnT()\n",
    "    tnt_pos_tagger.train(train_data)\n",
    "    return(tnt_pos_tagger.tag(localWordList))\n",
    "\n",
    "\n",
    "def contentWords(pos_info, flag):\n",
    "    content = []\n",
    "    if flag:\n",
    "        for words in pos_info:\n",
    "            if not (words[1] == 'VAUX' or words[1] == 'SYM' or words[1] =='VFM' or words[1] == 'CC' or words[1] == 'PRP' or\n",
    "                words[1] ==  'PUNC' or words[1] ==  'QF' or words[1] ==  'RB' or words[1] ==  'QW' or words[\n",
    "                1] ==  'RP' or words[1] ==  'PREP'):\n",
    "                content.append(words[0])\n",
    "    else:\n",
    "        for words in pos_info:\n",
    "            if (words[1] ==  'NN' or words[1] ==  'NNP' or words[1] ==  'Unk'):\n",
    "                content.append(words[0])\n",
    "    return(content)\n",
    "\n",
    "\n",
    "def morph_get(word):\n",
    "    f1=open(\"hi\",\"w\")\n",
    "    f1.write(word)\n",
    "    f1.close()\n",
    "    cmd=\"echo $HOME\"\n",
    "    bin_path=subprocess.check_output(cmd, shell=True)\n",
    "    cmd1=\"sh \"+bin_path.decode(\"utf-8\").strip()+\"/bin/hin_morph.sh < hi\"\n",
    "    result=subprocess.check_output(cmd1, shell=True)\n",
    "    return(result.decode(\"utf-8\"))\n",
    "\n",
    "def word_(word):\n",
    "    f1 = open('canonical',\"w\")\n",
    "    f1.write(word)\n",
    "    f1.close()\n",
    "    cmd=\"echo $HOME_anu_test\"\n",
    "    path = \"/Anu_data/canonical_form_dictionary\"\n",
    "    bin_path=subprocess.check_output(cmd, shell=True)\n",
    "    cmd1=bin_path.decode(\"utf-8\").strip()+path+\"/canonical_form.out < canonical\"\n",
    "    result=subprocess.check_output(cmd1, shell=True)\n",
    "    return(result.decode(\"utf8\"))\n",
    "\n",
    "\n",
    "con = WXC(order='utf2wx') \n",
    "con1 = WXC(order='wx2utf')\n",
    "\n",
    "def get_root(word):\n",
    "    morph_list =  list(set(map(lambda x: x.replace(\"$_\",\"\").split(\"<\")[0].strip(\"$\").strip(\"*\"), morph_get(word_(con.convert(word))).split(\"/\")[1:])))\n",
    "    final_morph=list(map(lambda x:con1.convert(x), morph_list))\n",
    "    return(final_morph)\n",
    "    \n",
    "\n",
    "def generateCueWordList(titleList):\n",
    "    cueWordList = []\n",
    "    for words in titleList:\n",
    "        if words in synonym_dictionary:\n",
    "            temp_word = synonym_dictionary[words]\n",
    "        else:\n",
    "            temp_word = []\n",
    "            temp_word1 = get_root(words)\n",
    "            for tem in temp_word1:\n",
    "                if tem in synonym_dictionary:\n",
    "                    temp_word.extend(synonym_dictionary[tem])\n",
    "        cueWordList.extend(temp_word)\n",
    "    return(cueWordList)\n",
    "\n",
    "\n",
    "def titleFeatures(sent, titleList):\n",
    "    count_title = 0\n",
    "    for each_word in sent:\n",
    "        if each_word in titleList:\n",
    "            count_title+=1\n",
    "    return(count_title/len(sent))\n",
    "\n",
    "def cueFeatures(sent, cueWordList):\n",
    "    count_cue = 0\n",
    "    for each_word in sent:\n",
    "        if each_word in cueWordList:\n",
    "            count_cue +=1\n",
    "    return(count_cue/len(sent))\n",
    "\n",
    "def mon_dayFeatures(sent):\n",
    "    count=0\n",
    "    for each_word in sent:\n",
    "        if each_word in month_day:\n",
    "            count+=1\n",
    "    return(count/len(sent))\n",
    "\n",
    "def computeTf(wordDict, sent):\n",
    "    tfDict={}\n",
    "    for word in sent:\n",
    "        wordDict[word]+=1\n",
    "    length = len(sent)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(length)\n",
    "    return(tfDict, wordDict)\n",
    "\n",
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / float(val))\n",
    "    return idfDict\n",
    "\n",
    "def computeTFIDF(tf, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tf.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return tfidf\n",
    "\n",
    "def properNounFeature(pos_w):\n",
    "    count_prop = 0\n",
    "    count_unk = 0\n",
    "    for items in pos_w:\n",
    "        if (items[1] ==  \"NNP\"):\n",
    "            count_prop+=1\n",
    "        if (items[1] ==  \"Unk\"):\n",
    "            count_unk+=1\n",
    "    return(count_prop/len(pos_w),count_unk/len(pos_w))\n",
    "\n",
    "def dateFeature(sent):\n",
    "    pattern=re.compile(r'((?:(?:(?:(?:(?:[012]\\d{1})|[1-9])[-\\/]){2}(?:[12]\\d{3}|\\d{2}))|(?:[01]?\\d{1}[-\\/])(?:30|31)[-\\/](?:[12]\\d{3}|\\d{2})|(?:(?:[012]\\d{1})|[1-9])[-\\/][12]\\d{3}|(?:(?:\\d{1,2}\\s)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\\.|,|\\s|-)?(?:\\d{1,2})?(?:st|th|rd|nd)?(?:,|\\s|-)?\\s?[[12]\\d{3}|\\d{2}])|(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\\.|,|\\s|-)?\\s?(?:\\d{1,2})(?:st|th|rd|nd)?(?:,|\\s|-)?\\s?[[12]\\d{3}|\\d{2}]))|[12]{1}\\d{3})')\n",
    "    pat=re.findall(pattern, sent)\n",
    "    return(len(pat)/len(sent))\n",
    "\n",
    "def sent_position():\n",
    "    l=len(tokenized_list)\n",
    "    position=[0]*l\n",
    "    if l%2==0:\n",
    "        mid=int(l/2)\n",
    "    else:\n",
    "        mid=int((l+1)/2 )  \n",
    "    for i in range(0,mid):\n",
    "        position[i]=l-i\n",
    "        position[l-i-1]=l-i\n",
    "    return(position)\n",
    "\n",
    "def englishLoanwordsFeature(sent):\n",
    "    count=0\n",
    "    for i in sent:\n",
    "        if i in english_loanwords:\n",
    "            count+=1\n",
    "    return(count/len(sent))\n",
    "\n",
    "def hasNumbers(inputString):\n",
    "     return sum([1 if char.isdigit() else 0 for char in inputString])/len(inputString)\n",
    "\n",
    "orig_title = input_text.split(\"\\n\")[0].strip()\n",
    "title= remove_stopwords(word_tokenize(orig_title.translate(str.maketrans('', '', punctuation.replace(\"-\",\"\").replace(\"'\",\"\").replace('\"',\"\")))))\n",
    "titleList = contentWords(partsOfSpeechTagger(title),0)\n",
    "cueWordList = generateCueWordList(titleList)\n",
    "rest_text_ = input_text.strip(orig_title)\n",
    "rest_text= rest_text_.translate(str.maketrans('', '', punctuation)).replace(u\"\\xa0\",\"\")\n",
    "segmented_text =  rest_text.strip().split(\"।\")\n",
    "try:\n",
    "    segmented_text.remove(\"\")\n",
    "except:\n",
    "    print(\"nothing\")\n",
    "    \n",
    "originalFile = copy.deepcopy(rest_text)\n",
    "originalSentenceList = [sentenceList.strip() for sentenceList in rest_text_.split(\"।\")]\n",
    "tokenized_list = [remove_stopwords(word_tokenize(i)) for i in segmented_text]\n",
    "index_sent =  {\" \".join(sen):loc for loc, sen in enumerate(tokenized_list)}\n",
    "bigramsDict , bigramsWordsList, bigram = generateBigrams(tokenized_list)\n",
    "bigramCountList = [countBigrams(i) for i in tokenized_list]\n",
    "title_feature = [titleFeatures(sent, titleList) for sent in tokenized_list]\n",
    "cue_feature = [cueFeatures(sent,cueWordList) for sent in tokenized_list]\n",
    "month_day_feature = [mon_dayFeatures(sent) for sent in tokenized_list]\n",
    "vocab=set(reduce(lambda x,y:x+y,tokenized_list))\n",
    "wordDict = dict.fromkeys(vocab, 0) \n",
    "tfword=[computeTf(wordDict.copy(),sent) for sent in tokenized_list]\n",
    "docList=[i[1] for i in tfword]\n",
    "tf=[i[0] for i in tfword]\n",
    "idfs=computeIDF(docList)\n",
    "tfidf=[computeTFIDF(tf_i, idfs) for tf_i in tf]\n",
    "tfidf_average = [sum(j.values())/len([e for e in j.values() if e]) for j in tfidf]\n",
    "prt_speech = [partsOfSpeechTagger(sent) for sent in tokenized_list]\n",
    "noun_feature = [properNounFeature(i) for i in prt_speech]\n",
    "prp_feature= [i[0] for i in noun_feature]\n",
    "unk_feature= [i[1] for i in noun_feature]\n",
    "date_feature = [dateFeature(i) for i in segmented_text]\n",
    "position_feature = sent_position()\n",
    "eng_loan = [englishLoanwordsFeature(sent) for sent in tokenized_list]\n",
    "digit = [hasNumbers(sent) for sent in tokenized_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sum_features = [0]*len(tokenized_list)\n",
    "for loc,sent in enumerate(tokenized_list):\n",
    "    sum_features[loc] = 10*title_feature[loc]+9*cue_feature[loc]+month_day_feature[loc]+bigramCountList[loc]+tfidf_average[loc]+6*prp_feature[loc]+unk_feature[loc]+8*date_feature[loc]+position_feature[loc]+eng_loan[loc]+digit[loc]\n",
    "    \n",
    "\n",
    "sum_features = np.array(sum_features)\n",
    "ids = (-sum_features).argsort()[:int(np.ceil(len(tokenized_list)*percentage))]\n",
    "ids=sorted(ids)\n",
    "summary=[]\n",
    "for i in ids:\n",
    "    summary.append(originalSentenceList[i].replace(u\"\\xa0\",\"\")+\"।\")\n",
    "    \n",
    "final_summary = orig_title+\"\\n\"+\" \".join(summary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक घंटा 45 मिनट तक खून जैसा लाल दिखेगा चांद, दुनिया देखेगी सदी का सबसे लंबा ग्रहण\n",
      "\n",
      "आगामी शुक्रवार वह दिन होगा जो सदियों तक याद रखा जाएगा। इस दिन दुनिया सदी का सबसे लंबा चंद्र ग्रहण देखेगी जो एक घंटा 45 मिनट तक रहेगा। यह दिन खगोलशास्त्रियों के लिए भी खास होने जा रहा है। 27 जुलाई की मध्यरात्रि में लगने जा रहे इस खगोलीय घटना में करीब चार घंटे तक चन्द्रमा इस ग्रहण के प्रभाव में रहेगा। इस दिन मंगल भी पृथ्वी के काफी करीब आने वाला है। चंद्र ग्रहण को लेकर पूरे देश के खगोलशास्त्री काफी उत्साहित हैं। \n",
      "\n",
      "यह एक ऐसी घटना है जब दुनियाभर के स्टार गैजर को रक्त जैसे लाल चंद्रमा को देखने का मौका मिलेगा। यह स्थिति तब उत्पन्न होती है जब चंद्रमा पूरी तरह से ग्रहण में होता है और सूरज की रोशनी के कारण लाल दिखाई देने लगता है।\n",
      "\n",
      "अस्ट्रेलियन नेशनल यूनिवर्सिटी के रिसर्च स्कूल ऑफ एस्ट्रोनोमी और एस्ट्रेफिजिक्स के एस्ट्रोनोमर ब्रैड टकर  ने बताया कि चंद्रमा हमेशा सूर्य और पृथ्वी के साथ पूर्ण संरेखण में नहीं होता है इसलिए हमें हर चंद्र चक्र में हमें चंद्र ग्रहण देखने को नहीं मिलता है। उन्होंने आगे कहा कि हम देखते हैं कि सूर्य चंद्रमा 35000 किमी दूर रहने के बाद भी सूर्योदय और सूर्यास्त के समय चंद्रमा की सतह को रोशनी देता है। अगर आप चंद्रमा पर हैं तो आपको सूर्य ग्रहण देखने का मौका मिलता है जब पृथ्वी सूर्य के सीध में आता है और वह पूरी तरह से ढक जाता है।\n",
      "\n",
      "यह चंद्र ग्रहण इसलिए भी खास है क्योंकि यह पूरी दुनिया में दिखाई देगा सिर्फ उत्तरी अमेरिका को छोड़कर। लेकिन यह सबसे खूबसूरत और अच्छा दिखेगा अस्ट्रेलिया, न्यूजीलैंड, यूरोप, अफ्रीका और एशिया में। इस वर्ष यह दूसरा मौका होगा जब ग्रहण के समय ब्लड मून दिखेगा। \n",
      "\n",
      "खगोलशास्त्री बताते हैं कि चंद्र ग्रहण को  ‘ब्लड मून’ कहे जाने की वजह इसका रंग होता है जो इस दौरान पूरी तरह से बदल जाता है। दरअसल, चंद्रग्रहण के समय जब सूरज और चंद्रमा के बीच पृथ्वी आती है तो सूरज की किरण रुक जाती है। पृथ्वी के वातावरण की वजह से रोशनी मुड़कर चांद पर पड़ती है और इस वजह से यह लाल नजर आता है। जब पूर्ण चंद्रग्रहण होता है तभी ब्लड मून होता है।\n",
      "\n",
      "इतना लंबा ग्रहण करीब 150 साल बाद दिखाई देने जा रहा है। यह आरंभ रात 11 बजकर 54 मिनट पर होगा इसका मध्यकाल रात 1 बजकर 54 मिनट पर होगा और 28 जुलाई को सुबह 3 बजकर 49 मिनट पर ग्रहण का समाप्त होगा। इस तरह ग्रहण की कुल अवधि 3 घंटे 55 मिनट की होगी।\n"
     ]
    }
   ],
   "source": [
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "एक घंटा 45 मिनट तक खून जैसा लाल दिखेगा चांद, दुनिया देखेगी सदी का सबसे लंबा ग्रहण\n",
      "आगामी शुक्रवार वह दिन होगा जो सदियों तक याद रखा जाएगा। इस दिन दुनिया सदी का सबसे लंबा चंद्र ग्रहण देखेगी जो एक घंटा 45 मिनट तक रहेगा। 27 जुलाई की मध्यरात्रि में लगने जा रहे इस खगोलीय घटना में करीब चार घंटे तक चन्द्रमा इस ग्रहण के प्रभाव में रहेगा। चंद्र ग्रहण को लेकर पूरे देश के खगोलशास्त्री काफी उत्साहित हैं। पृथ्वी के वातावरण की वजह से रोशनी मुड़कर चांद पर पड़ती है और इस वजह से यह लाल नजर आता है। जब पूर्ण चंद्रग्रहण होता है तभी ब्लड मून होता है। इतना लंबा ग्रहण करीब 150 साल बाद दिखाई देने जा रहा है। यहआरंभ रात 11 बजकर 54 मिनट पर होगा इसका मध्यकाल रात 1 बजकर 54 मिनट पर होगा और 28 जुलाई को सुबह 3 बजकर 49 मिनट पर ग्रहण कासमाप्तहोगा। इस तरह ग्रहण की कुल अवधि 3 घंटे 55 मिनट की होगी।\n"
     ]
    }
   ],
   "source": [
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(np.ceil(len(tokenized_list)*0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02702702702702703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cue_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "month_day_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.004329004329004329,\n",
       " 0.012987012987012988,\n",
       " 0.0,\n",
       " 0.004329004329004329,\n",
       " 0.004329004329004329,\n",
       " 0.004329004329004329,\n",
       " 0.004329004329004329,\n",
       " 0.008658008658008658]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigramCountList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012133943116785863,\n",
       " 0.017391387694478136,\n",
       " 0.04626387040077326,\n",
       " 0.020283215710739017,\n",
       " 0.03973620601272673,\n",
       " 0.04868028327937198,\n",
       " 0.0513253720776522,\n",
       " 0.0190496180175016]"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.013513513513513514, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prp_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6351351351351351,\n",
       " 0.5813953488372093,\n",
       " 0.5882352941176471,\n",
       " 0.5641025641025641,\n",
       " 0.55,\n",
       " 0.8666666666666667,\n",
       " 0.7272727272727273,\n",
       " 0.6818181818181818]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0031746031746031746, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package indian to /home/akanksha/nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-72df7e833f54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mcon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWXC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf2wx'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mcon1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWXC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'wx2utf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mmorph_list\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$_\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mfinal_morph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcon1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wxconv/wx_format.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"ssf\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             self.special = set(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wxconv/wx.py\u001b[0m in \u001b[0;36mutf2wx\u001b[0;34m(self, unicode_)\u001b[0m\n\u001b[1;32m   2920\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mutf2wx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2921\u001b[0m         \u001b[0;34m\"\"\"Convert UTF string to WX-Roman\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2922\u001b[0;31m         \u001b[0municode_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2923\u001b[0m         \u001b[0;31m# Mask iscii characters (if any)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/wxconv/wx.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1796\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mZERO_WIDTH_NON_JOINER\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mZERO_WIDTH_JOINER\u001b[0m \u001b[0mremoval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1797\u001b[0m         \"\"\"\n\u001b[0;32m-> 1798\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\uFEFF'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BYTE_ORDER_MARK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1799\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\uFFFE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BYTE_ORDER_MARK_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1800\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\u2060'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# WORD JOINER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "from wxconv import WXC\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "def morph_get(word):\n",
    "    f1=open(\"hi\",\"w\")\n",
    "    f1.write(word)\n",
    "    f1.close()\n",
    "    cmd=\"echo $HOME\"\n",
    "    bin_path=subprocess.check_output(cmd, shell=True)\n",
    "    cmd1=\"sh \"+bin_path.decode(\"utf-8\").strip()+\"/bin/hin_morph.sh < hi\"\n",
    "    result=subprocess.check_output(cmd1, shell=True)\n",
    "    return(result.decode(\"utf-8\"))\n",
    "\n",
    "def word_(word):\n",
    "    f1 = open('canonical',\"w\")\n",
    "    f1.write(word)\n",
    "    f1.close()\n",
    "    cmd=\"echo $HOME_anu_test\"\n",
    "    path = \"/Anu_data/canonical_form_dictionary\"\n",
    "    bin_path=subprocess.check_output(cmd, shell=True)\n",
    "    cmd1=bin_path.decode(\"utf-8\").strip()+path+\"/canonical_form.out < canonical\"\n",
    "    result=subprocess.check_output(cmd1, shell=True)\n",
    "    return(result.decode(\"utf8\"))\n",
    "\n",
    "\n",
    "\n",
    "con = WXC(order='utf2wx') \n",
    "con1 = WXC(order='wx2utf')\n",
    "\n",
    "def get_root(word)\n",
    "    morph_list =  list(set(map(lambda x: x.replace(\"$_\",\"\").split(\"<\")[0].strip(\"$\").strip(\"*\"), morph_get(word_(con.convert(word))).split(\"/\")[1:])))\n",
    "    final_morph=list(map(lambda x:con1.convert(x), morph_list))\n",
    "    return(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'लिखा': 1.3862943611198906,\n",
       "         'पाया': 2.0794415416798357,\n",
       "         'नवोदय': 2.0794415416798357,\n",
       "         'पढ़': 2.0794415416798357,\n",
       "         'भी': 0.9808292530117262,\n",
       "         'मेरे': 1.3862943611198906,\n",
       "         'मरा': 2.0794415416798357,\n",
       "         'वर्ष': 1.3862943611198906,\n",
       "         'हां': 2.0794415416798357,\n",
       "         'गयी': 2.0794415416798357,\n",
       "         '7': 2.0794415416798357,\n",
       "         'विद्यालय': 2.0794415416798357,\n",
       "         'लेकर': 1.3862943611198906,\n",
       "         'कक्षा': 2.0794415416798357,\n",
       "         'नीरज': 0.13353139262452257,\n",
       "         'मर': 2.0794415416798357,\n",
       "         'दास': 2.0794415416798357,\n",
       "         'आत्मविश्वास': 2.0794415416798357,\n",
       "         'सकूंगा': 2.0794415416798357,\n",
       "         'हिंदी': 1.3862943611198906,\n",
       "         'अंदर': 1.3862943611198906,\n",
       "         'पुस्तक': 2.0794415416798357,\n",
       "         'बहाने': 2.0794415416798357,\n",
       "         'स्वयं': 2.0794415416798357,\n",
       "         'सपनो': 2.0794415416798357,\n",
       "         'पढ़कर': 2.0794415416798357,\n",
       "         'परिचय': 2.0794415416798357,\n",
       "         'समय': 1.3862943611198906,\n",
       "         'जी': 0.13353139262452257,\n",
       "         'कविता': 0.9808292530117262,\n",
       "         'छात्र': 2.0794415416798357,\n",
       "         'थाउस': 2.0794415416798357,\n",
       "         'मैं': 0.9808292530117262,\n",
       "         'गोपाल': 2.0794415416798357,\n",
       "         'छुप': 2.0794415416798357,\n",
       "         'सोचा': 2.0794415416798357,\n",
       "         'कभी': 2.0794415416798357,\n",
       "         'पढ़ाई': 2.0794415416798357,\n",
       "         'वालो': 2.0794415416798357,\n",
       "         'जाने': 2.0794415416798357,\n",
       "         'तपस्वी': 2.0794415416798357,\n",
       "         'पाठ्य': 2.0794415416798357,\n",
       "         'लुटाने': 2.0794415416798357,\n",
       "         'भिंड': 2.0794415416798357,\n",
       "         'कवि': 0.6931471805599453,\n",
       "         'नीरजजी': 2.0794415416798357,\n",
       "         'जीवन': 2.0794415416798357,\n",
       "         '13': 2.0794415416798357,\n",
       "         'अनुभूति': 2.0794415416798357,\n",
       "         'महामानव': 2.0794415416798357,\n",
       "         'आयु': 2.0794415416798357,\n",
       "         'मंच': 0.6931471805599453,\n",
       "         'जन्म': 2.0794415416798357,\n",
       "         'गर्व': 2.0794415416798357,\n",
       "         'वालोजीवन': 2.0794415416798357,\n",
       "         'इटावा': 1.3862943611198906,\n",
       "         'नही': 0.9808292530117262,\n",
       "         'जीवनी': 2.0794415416798357,\n",
       "         'पहले': 2.0794415416798357,\n",
       "         'व्यर्थ': 2.0794415416798357,\n",
       "         'अश्रु': 2.0794415416798357,\n",
       "         'पंक्तियों': 2.0794415416798357,\n",
       "         'प्राप्त': 2.0794415416798357,\n",
       "         'इन्ही': 2.0794415416798357,\n",
       "         'महोत्सव': 2.0794415416798357,\n",
       "         'सौभाग्य': 1.3862943611198906,\n",
       "         'आठवीं': 2.0794415416798357,\n",
       "         'मस्तिष्क': 2.0794415416798357,\n",
       "         'बार': 1.3862943611198906,\n",
       "         'रहासन': 2.0794415416798357,\n",
       "         'सैफई': 2.0794415416798357,\n",
       "         'शायद': 2.0794415416798357,\n",
       "         'पहली': 2.0794415416798357,\n",
       "         'कविताएं': 2.0794415416798357,\n",
       "         'सदैव': 0.9808292530117262,\n",
       "         'प्रेरणा': 2.0794415416798357,\n",
       "         '2005': 2.0794415416798357,\n",
       "         'लिखना': 2.0794415416798357,\n",
       "         'आया': 2.0794415416798357,\n",
       "         'संघर्ष': 2.0794415416798357,\n",
       "         'मन': 1.3862943611198906,\n",
       "         'असर': 2.0794415416798357,\n",
       "         'जगाईकक्षा': 2.0794415416798357,\n",
       "         'वो': 0.9808292530117262,\n",
       "         'मैंने': 2.0794415416798357,\n",
       "         'चरण': 2.0794415416798357,\n",
       "         'छूने': 2.0794415416798357,\n",
       "         'जिसने': 1.3862943611198906,\n",
       "         'नाम': 1.3862943611198906,\n",
       "         'शुरू': 2.0794415416798357,\n",
       "         'उन्होंने': 2.0794415416798357,\n",
       "         'पास': 2.0794415416798357,\n",
       "         'बुलाया': 2.0794415416798357,\n",
       "         'नवोदित': 2.0794415416798357,\n",
       "         'काव्यपाठ': 2.0794415416798357,\n",
       "         'पीठ': 2.0794415416798357,\n",
       "         'करोगे': 2.0794415416798357,\n",
       "         'थपथपाते': 2.0794415416798357,\n",
       "         'इटावे': 2.0794415416798357,\n",
       "         'सम्मुख': 2.0794415416798357,\n",
       "         'मेरी': 2.0794415416798357,\n",
       "         'रूप': 2.0794415416798357,\n",
       "         'रोशन': 2.0794415416798357,\n",
       "         'फितरत': 2.0794415416798357,\n",
       "         'मुलाकात': 2.0794415416798357,\n",
       "         'सम्मान': 2.0794415416798357,\n",
       "         'स्नान': 2.0794415416798357,\n",
       "         'मंदिर': 2.0794415416798357,\n",
       "         '2': 2.0794415416798357,\n",
       "         'सूफियाना': 2.0794415416798357,\n",
       "         'दृष्टिकोण': 2.0794415416798357,\n",
       "         'मिलता': 2.0794415416798357,\n",
       "         'आगरा': 2.0794415416798357,\n",
       "         'रहानीरज': 2.0794415416798357,\n",
       "         'अल्हण': 2.0794415416798357,\n",
       "         'हर': 1.3862943611198906,\n",
       "         'राष्ट्रीय': 2.0794415416798357,\n",
       "         'अब': 2.0794415416798357,\n",
       "         'लगानीरज': 2.0794415416798357,\n",
       "         'मंचो': 2.0794415416798357,\n",
       "         'आखिरी': 2.0794415416798357,\n",
       "         'आशीष': 2.0794415416798357,\n",
       "         'मदिर': 2.0794415416798357,\n",
       "         'तब': 2.0794415416798357,\n",
       "         'पूर्व': 2.0794415416798357,\n",
       "         'दर्जन': 2.0794415416798357,\n",
       "         'बरसात': 2.0794415416798357,\n",
       "         'पढ़ना': 2.0794415416798357,\n",
       "         'कम': 2.0794415416798357,\n",
       "         'गीतों': 2.0794415416798357,\n",
       "         'भर': 2.0794415416798357,\n",
       "         'लिखने': 2.0794415416798357,\n",
       "         'कहीं': 2.0794415416798357,\n",
       "         'ठीक': 2.0794415416798357,\n",
       "         'सामायिक': 2.0794415416798357,\n",
       "         'तबसे': 2.0794415416798357,\n",
       "         'स्मरण': 2.0794415416798357,\n",
       "         'बात': 2.0794415416798357,\n",
       "         'आशीर्वाद': 2.0794415416798357,\n",
       "         'दियाबस': 2.0794415416798357,\n",
       "         'चला': 2.0794415416798357,\n",
       "         'उन्हें': 2.0794415416798357,\n",
       "         'लगा': 2.0794415416798357,\n",
       "         'हूं': 2.0794415416798357,\n",
       "         'ठाक': 2.0794415416798357,\n",
       "         'पता': 2.0794415416798357,\n",
       "         'मुद्दों': 2.0794415416798357,\n",
       "         'पुनः': 2.0794415416798357,\n",
       "         'जाकर': 2.0794415416798357,\n",
       "         'गीत': 2.0794415416798357,\n",
       "         'चलन': 2.0794415416798357,\n",
       "         'ढूंढना': 2.0794415416798357,\n",
       "         'तृप्ति': 2.0794415416798357,\n",
       "         'ब्राउजर': 2.0794415416798357,\n",
       "         'इंटरनेट': 2.0794415416798357,\n",
       "         'पढ़करसुनकर': 2.0794415416798357,\n",
       "         'बन': 2.0794415416798357,\n",
       "         'मोबाइल': 2.0794415416798357,\n",
       "         'फिल्मी': 2.0794415416798357,\n",
       "         'जीवित': 2.0794415416798357,\n",
       "         'रहेंगे': 1.3862943611198906,\n",
       "         'सिर्फ': 2.0794415416798357,\n",
       "         'त्यागा': 2.0794415416798357,\n",
       "         'जगत': 1.3862943611198906,\n",
       "         'शरीर': 2.0794415416798357,\n",
       "         'माथे': 2.0794415416798357,\n",
       "         'काव्य': 2.0794415416798357,\n",
       "         'पा': 2.0794415416798357,\n",
       "         'लिया': 2.0794415416798357,\n",
       "         'अमिट': 2.0794415416798357,\n",
       "         'कतई': 2.0794415416798357,\n",
       "         'हमारे': 2.0794415416798357,\n",
       "         'मानव': 2.0794415416798357,\n",
       "         'कराती': 2.0794415416798357,\n",
       "         'अमर': 2.0794415416798357,\n",
       "         'श्रृंगार': 2.0794415416798357,\n",
       "         'बनकर': 2.0794415416798357,\n",
       "         'भाग्य': 2.0794415416798357,\n",
       "         'दोहे': 2.0794415416798357,\n",
       "         'मानता': 2.0794415416798357,\n",
       "         'बीच': 2.0794415416798357,\n",
       "         'अहसास': 2.0794415416798357,\n",
       "         'होना': 2.0794415416798357,\n",
       "         'बिंदी': 2.0794415416798357,\n",
       "         'पंक्ति': 2.0794415416798357,\n",
       "         'दैवीय': 2.0794415416798357,\n",
       "         'रहेगी': 2.0794415416798357})"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateIdf(tokenized_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'मर'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con1 = WXC(order='wx2utf') \n",
    "con1.convert('mara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=set(reduce(lambda x,y:x+y,tokenized_list))\n",
    "wordDict = dict.fromkeys(vocab, 0) \n",
    "\n",
    "\n",
    "tfword=[computeTf(wordDict.copy(),sent) for sent in tokenized_list]\n",
    "docList=[i[1] for i in tfword]\n",
    "tf=[i[0] for i in tfword]\n",
    "idfs=computeIDF(docList)\n",
    "\n",
    "\n",
    "tfidf=[computeTFIDF(tf_i, idfs) for tf_i in tf]\n",
    "tfidf_average = [sum(j.values())/len([e for e in j.values() if e]) for j in tfidf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.012133943116785863,\n",
       " 0.017391387694478136,\n",
       " 0.04626387040077326,\n",
       " 0.020283215710739017,\n",
       " 0.03973620601272673,\n",
       " 0.04868028327937198,\n",
       " 0.0513253720776522,\n",
       " 0.0190496180175016]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "data=\"dsmk kks 7\"\n",
    "pattern=re.compile(r'((?:(?:(?:(?:(?:[012]\\d{1})|[1-9])[-\\/]){2}(?:[12]\\d{3}|\\d{2}))|(?:[01]?\\d{1}[-\\/])(?:30|31)[-\\/](?:[12]\\d{3}|\\d{2})|(?:(?:[012]\\d{1})|[1-9])[-\\/][12]\\d{3}|(?:(?:\\d{1,2}\\s)?(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\\.|,|\\s|-)?(?:\\d{1,2})?(?:st|th|rd|nd)?(?:,|\\s|-)?\\s?[[12]\\d{3}|\\d{2}])|(?:(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*(?:\\.|,|\\s|-)?\\s?(?:\\d{1,2})(?:st|th|rd|nd)?(?:,|\\s|-)?\\s?[[12]\\d{3}|\\d{2}]))|[12]{1}\\d{3})')\n",
    "re.findall(pattern, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_position():\n",
    "    l=len(tokenized_list)\n",
    "    position=[0]*l\n",
    "    if l%2==0:\n",
    "        mid=int(l/2)\n",
    "    else:\n",
    "        mid=int((l+1)/2 )  \n",
    "    for i in range(0,mid):\n",
    "        position[i]=l-i\n",
    "        position[l-i-1]=l-i\n",
    "    return(position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 8, 7, 6, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"support_data/english_romancized.txt\",\"r\") as g:\n",
    "    d=set(g.read().strip().splitlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.discard(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=[]\n",
    "for i in d:\n",
    "    df.append(i.split(\",\")[0]+\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = WXC(order='utf2wx') \n",
    "con1 = WXC(order='wx2utf')\n",
    "data_engl=[]\n",
    "for i in df:\n",
    "    t=i.split()\n",
    "    if len(t)>1:\n",
    "        for k in t[:-1]:\n",
    "            l=con1.convert(k+\"a\")\n",
    "            data_engl.append(l)\n",
    "        l=con1.convert(t[-1])\n",
    "        data_engl.append(l)\n",
    "    else:\n",
    "        l=con1.convert(i)\n",
    "        data_engl.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engl=set(data_engl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(ए.ग्.अ',\n",
       " '(एन्डेएटेएवेए)अ',\n",
       " '(ऐ/H)अ',\n",
       " '(क्हेलोण्)अ',\n",
       " '(नोठ्स्)अ',\n",
       " 'अ',\n",
       " 'अअ',\n",
       " 'अअकअस्ह',\n",
       " 'अअप्रेस्हन',\n",
       " 'अअम्लेट',\n",
       " 'अअर्किटेक्ट',\n",
       " 'अअर्केस्ट्रअअ',\n",
       " 'अअर्जेन्टेएनअअ',\n",
       " 'अइप',\n",
       " 'अइस्क्रेएम',\n",
       " 'अइस्च्रिम',\n",
       " 'अइस्ड',\n",
       " 'अउङ',\n",
       " 'अउट',\n",
       " 'अउट्!अ',\n",
       " 'अउप्रेस्हन',\n",
       " 'अउप्ह',\n",
       " 'अउर्ढर',\n",
       " 'अएए.ए.एस्स्.अ',\n",
       " 'अएए.एस्.अएए.अ',\n",
       " 'अएए.सेए.योओ.अ',\n",
       " 'अएए.सेए.सेए.अ',\n",
       " 'अएएअएएटेएअ',\n",
       " 'अकउण्ट',\n",
       " 'अक्टोओबर',\n",
       " 'अक्सिजअन',\n",
       " 'अटक',\n",
       " 'अट्होरिटेएअ',\n",
       " 'अपेएल',\n",
       " 'अप्रइल',\n",
       " 'अम्पयर',\n",
       " 'अल्टेएमेटम',\n",
       " 'इण्कम',\n",
       " 'इण्ग्लइण्ड',\n",
       " 'इण्ग्लइन्ड',\n",
       " 'इण्ङउर्म',\n",
       " 'इण्जनेएयर',\n",
       " 'इण्जेएनियर',\n",
       " 'इण्जेक्स्हन',\n",
       " 'इण्टर्नट',\n",
       " 'इण्टर्नेट',\n",
       " 'इण्डइक्स',\n",
       " 'इण्डस्ट्रेएअ',\n",
       " 'इण्डियअअ',\n",
       " 'इण्ढियअअ',\n",
       " 'इण्वइट',\n",
       " 'इन्ङोर्म',\n",
       " 'इन्प्ह्लेस्हन',\n",
       " 'इमेज',\n",
       " 'इयर्पोर्ट',\n",
       " 'इलक्ट्रनिक',\n",
       " 'ईन्डेएयअअ',\n",
       " 'उन्ङेर',\n",
       " 'एए-मेल',\n",
       " 'एएरक',\n",
       " 'एकउण्ठ',\n",
       " 'एक्टर्स',\n",
       " 'एक्पट्रअअ',\n",
       " 'एक्स्प्रेस',\n",
       " 'एक्स्रेअ',\n",
       " 'एङ्डेएअएएअ',\n",
       " 'एच्हइवेएअ',\n",
       " 'एजेन्सेएअ',\n",
       " 'एट्च्.अ',\n",
       " 'एठेएएम',\n",
       " 'एडिटर',\n",
       " 'एडिस्हअन',\n",
       " 'एड्मिस्हन',\n",
       " 'एड्स',\n",
       " 'एन्.डेए.ए.अ',\n",
       " 'एन्.डेए.टेए.वेए.अ',\n",
       " 'एन्जन',\n",
       " 'एमेअ',\n",
       " 'एम्बुलेण्स',\n",
       " 'एयर',\n",
       " 'एयर्पोर्ट',\n",
       " 'एयर्लइन्(स्)अ',\n",
       " 'एल्ङअअ',\n",
       " 'एल्बम',\n",
       " 'एसेम्ब्लेएअ',\n",
       " 'एस्.अएए.टेए.अ',\n",
       " 'एस्.पेएअ',\n",
       " 'एस्हेएयअ(न्)अ',\n",
       " 'ओङिस',\n",
       " 'ओण्लएएन',\n",
       " 'ओपेरस्हन',\n",
       " 'ओपेरेटर',\n",
       " 'ओलम्पिक',\n",
       " 'ओवेर्ब्रिड्गेअ',\n",
       " 'ओवेर्होलिण्ग',\n",
       " 'ओवेस्हन',\n",
       " 'कंपनेएअ',\n",
       " 'कअन्ङ्रेन्स',\n",
       " 'कअर',\n",
       " 'कअर्ढ',\n",
       " 'कअर्परेस्हन',\n",
       " 'कइन्सर',\n",
       " 'कइबिनेट',\n",
       " 'कइम्रअ',\n",
       " 'कइम्रअअ',\n",
       " 'कइम्रमन',\n",
       " 'कइरियर',\n",
       " 'कइस्ह',\n",
       " 'कइस्ह्लेस',\n",
       " 'कङन',\n",
       " 'कण्ग्रेस',\n",
       " 'कण्स्टेबल',\n",
       " 'कन्ट्रोल',\n",
       " 'कप',\n",
       " 'कप्टअन',\n",
       " 'कमअन',\n",
       " 'कमण्डोअ',\n",
       " 'कमिटेएअ',\n",
       " 'कमिस्ह्नर',\n",
       " 'कमेएस्हअन',\n",
       " 'कमेएस्हनर',\n",
       " 'कम्पोओटर',\n",
       " 'कम्प्योओटर',\n",
       " 'कम्युनिकेस्हण्स',\n",
       " 'कम्युनिस्ट',\n",
       " 'करण्सेएअ',\n",
       " 'करेएयर',\n",
       " 'करेण',\n",
       " 'करेण्सेएअ',\n",
       " 'कर्टोओन',\n",
       " 'कर्ड',\n",
       " 'कर्नअअ',\n",
       " 'कर्नल',\n",
       " 'कलेक्टर',\n",
       " 'कव्रेज',\n",
       " 'किलर',\n",
       " 'किलोमेएटर',\n",
       " 'कुट्टअ',\n",
       " 'केट्रिन्ग',\n",
       " 'केस',\n",
       " 'कोअ',\n",
       " 'कोट',\n",
       " 'कोटोन',\n",
       " 'कोन्स्टेबल',\n",
       " 'कोप्य',\n",
       " 'कोमन्वेल्ट्ह',\n",
       " 'कोमेडेएअ',\n",
       " 'कोर्ट',\n",
       " 'कोर्पोरेट',\n",
       " 'कोर्पोरेस्हन',\n",
       " 'क्रइम',\n",
       " 'क्रिकट',\n",
       " 'क्रिस्ह्च्ह्यन',\n",
       " 'क्रेढिट',\n",
       " 'क्लअस',\n",
       " 'क्लर्केएअ',\n",
       " 'क्लिप',\n",
       " 'क्लेएनर',\n",
       " 'क्वअलिङअएएअ',\n",
       " 'क्विक',\n",
       " 'गइस',\n",
       " 'गटिअ',\n",
       " 'गर्ड',\n",
       " 'गवर्नर',\n",
       " 'गेम',\n",
       " 'गोओगल',\n",
       " 'गोडअम',\n",
       " 'गोबर',\n",
       " 'ग्लोबल',\n",
       " 'ङअइल',\n",
       " 'ङअयरिण्ग',\n",
       " 'ङक्टरेएअ',\n",
       " 'ङन',\n",
       " 'ङिल्म',\n",
       " 'ङुटेज',\n",
       " 'ङेएट',\n",
       " 'ङेस्बुक',\n",
       " 'ङोटोअ',\n",
       " 'ङोन',\n",
       " 'ङ्रअन्स',\n",
       " 'ङ्रन्ट',\n",
       " 'ङ्रेएलइण्स',\n",
       " 'च्हअर्ज',\n",
       " 'च्हइंपियन',\n",
       " 'च्हइक',\n",
       " 'च्हइकप',\n",
       " 'च्हइन',\n",
       " 'च्हएएअ',\n",
       " 'च्हर्ज',\n",
       " 'च्हलो!अ',\n",
       " 'च्हल्क',\n",
       " 'च्हुइण्गम',\n",
       " 'च्हेक',\n",
       " 'जअज',\n",
       " 'जअपअन',\n",
       " 'जनरल',\n",
       " 'जन्वअरेएअ',\n",
       " 'जर्मन',\n",
       " 'जुनेएयर',\n",
       " 'जुलएएअ',\n",
       " 'जेएअ',\n",
       " 'जेएपेएएस्स',\n",
       " 'जेन्रल',\n",
       " 'जेन्रेटर',\n",
       " 'जेपेएसेएअ',\n",
       " 'जेल',\n",
       " 'जेस्मिन',\n",
       " 'जोओन',\n",
       " 'जोकर',\n",
       " 'टइक्नोलोजेएअ',\n",
       " 'टइक्स',\n",
       " 'टइक्सेएअ',\n",
       " 'टइक्सेएवअलअअ',\n",
       " 'टइब्लेट',\n",
       " 'टदेएवअलअअ',\n",
       " 'टिकट',\n",
       " 'टिप्(स्)अ',\n",
       " 'टिप्:अ',\n",
       " 'टेँअ',\n",
       " 'टेए-पर्टेएअ',\n",
       " 'टेएअ',\n",
       " 'टेएबेएअ',\n",
       " 'टेएवेएअ',\n",
       " 'टेकअङ',\n",
       " 'टेक्नोलोजेएअ',\n",
       " 'टेण्डर',\n",
       " 'टेरअपेएअ',\n",
       " 'टेरपेएअ',\n",
       " 'टेस्ट',\n",
       " 'टोओअ',\n",
       " 'टोर्नअअ',\n",
       " 'टोस्ट',\n",
       " 'ट्योओनिस्हअअ',\n",
       " 'ट्योओब',\n",
       " 'ट्रइवल',\n",
       " 'ट्रन्स्पोन्डेन्स',\n",
       " 'ट्रयल',\n",
       " 'ट्रस्ट',\n",
       " 'ट्रुच्क',\n",
       " 'ट्रेड',\n",
       " 'ट्रेड्मर्क',\n",
       " 'ट्रेन',\n",
       " 'ट्रेनिण्ग',\n",
       " 'ट्विटर',\n",
       " 'ट्हएएलइण्ड',\n",
       " 'ठइक्स',\n",
       " 'ठेलेएकोम',\n",
       " 'डअइरेक्टर',\n",
       " 'डअलर',\n",
       " 'डयअरेएअ',\n",
       " 'डरअँअ',\n",
       " 'डिँअइन',\n",
       " 'डिक्टटर',\n",
       " 'डिग्रेएअ',\n",
       " 'डिसंबर',\n",
       " 'डिस्कओओन्ट',\n",
       " 'डिस्क्वोलेएप्हएएअ',\n",
       " 'डिस्टर्ब',\n",
       " 'डिस्मिस',\n",
       " 'डिस्ह',\n",
       " 'डुबएएअ',\n",
       " 'डेए.एम्.अ',\n",
       " 'डेए.एम्.के.अ',\n",
       " 'डेए.एम्.जेए.अ',\n",
       " 'डेए.जे.सेए.अ',\n",
       " 'डेए.जेए.एम्.अ',\n",
       " 'डेए.जेए.सेए.एअ',\n",
       " 'डेए.वेए.डेए.अ',\n",
       " 'डेए.सेए.पेए.अ',\n",
       " 'डेएँअल',\n",
       " 'डेएलर',\n",
       " 'डेड्लएएन',\n",
       " 'डेन्मर्क',\n",
       " 'डेप्योओटेएअ',\n",
       " 'डेस्क',\n",
       " 'डोक्टर',\n",
       " 'डोनेस्हन',\n",
       " 'ड्रएएवर',\n",
       " 'ड्रग्स',\n",
       " 'ड्रप्ह्ट',\n",
       " 'ड्रम',\n",
       " 'ड्रेनेज',\n",
       " 'ड्रेस',\n",
       " 'ढेएनोमिनेस्हन',\n",
       " 'ढेबिठ',\n",
       " 'ण्ढ्ठ्Vअ',\n",
       " 'नंबर',\n",
       " 'नउर्मलएएँअ',\n",
       " 'नवंबर',\n",
       " 'नेस्हनल',\n",
       " 'नोट',\n",
       " 'नोटिस',\n",
       " 'नोठ',\n",
       " 'नोठ्बण्डेएअ',\n",
       " 'नोर्मलिँएअ',\n",
       " 'न्योओयर्क',\n",
       " 'पअर्ट्नर',\n",
       " 'पअस',\n",
       " 'पअस्पोर्ट',\n",
       " 'पइप्लइन',\n",
       " 'परेड',\n",
       " 'पर्टेएअ',\n",
       " 'पिन',\n",
       " 'पिस्टउल',\n",
       " 'पुलिस',\n",
       " 'पेए.एम्.अ',\n",
       " 'पेएएम',\n",
       " 'पेएएलेअ',\n",
       " 'पेएएसेएअ',\n",
       " 'पेण्सिल',\n",
       " 'पेण्स्हन',\n",
       " 'पेन',\n",
       " 'पेन्सिल',\n",
       " 'पेन्स्हन्;अ',\n",
       " 'पेन्स्हन्डरेएअ',\n",
       " 'पेपर',\n",
       " 'पोप',\n",
       " 'पोलइण्ड',\n",
       " 'पोस्ट्मोर्टम',\n",
       " 'प्रउपर्ठेएअ',\n",
       " 'प्रड्योओसर',\n",
       " 'प्रमोट',\n",
       " 'प्रिण्ट',\n",
       " 'प्रेस्हर',\n",
       " 'प्रोग्रम',\n",
       " 'प्रोङेसर',\n",
       " 'प्रोजेक्ट',\n",
       " 'प्रोमोअ',\n",
       " 'प्रोसेएजर',\n",
       " 'प्लअस्टिक',\n",
       " 'प्लग',\n",
       " 'प्लेट्ङर्म',\n",
       " 'प्हअस्प्हरस',\n",
       " 'प्हर्वरेएअ',\n",
       " 'प्होरेन्सिक',\n",
       " 'बँअअर',\n",
       " 'बअबोओअ',\n",
       " 'बअरिस्टर',\n",
       " 'बइग',\n",
       " 'बइण्क',\n",
       " 'बइलेण्स',\n",
       " 'बउलेएवुड',\n",
       " 'बक्लस्ह',\n",
       " 'बम',\n",
       " 'बस',\n",
       " 'बिँनस',\n",
       " 'बिटुमन',\n",
       " 'बिल',\n",
       " 'बिल्डर',\n",
       " 'बिल्डिण्ग',\n",
       " 'बिल्यन',\n",
       " 'बेएएसेङ',\n",
       " 'बेएजेपेएअ',\n",
       " 'बेएबेएसेएअ',\n",
       " 'बेस्बउल',\n",
       " 'बोइण्ग',\n",
       " 'बोक्स',\n",
       " 'बोलेएवुड',\n",
       " 'ब्.अ',\n",
       " 'ब्रिटेन',\n",
       " 'ब्रेक',\n",
       " 'ब्लइक्मेल',\n",
       " 'ब्लड',\n",
       " 'ब्लेम',\n",
       " 'ब्लोओअ',\n",
       " 'ब्हवन',\n",
       " 'मअर्ग',\n",
       " 'मअर्च्ह',\n",
       " 'मअर्बल',\n",
       " 'मइच्ह्ङिक्सिण्ग',\n",
       " 'मइट्ह्स',\n",
       " 'मइण्ग्नेएँअ',\n",
       " \"मइनजर्'अ\",\n",
       " 'मएएअ',\n",
       " 'मजिस्ट्रेट',\n",
       " 'मण्ट्रेएअ',\n",
       " 'मनेएअ',\n",
       " 'मर्कट',\n",
       " 'मर्ग',\n",
       " 'मस्हेएन',\n",
       " 'मिस्हनइरेएअ',\n",
       " 'मेएटर',\n",
       " 'मेएडियअ',\n",
       " 'मेकोवर',\n",
       " 'मेजर',\n",
       " 'मेट्रोअ',\n",
       " 'मेम्बर',\n",
       " 'मेयर',\n",
       " 'मेल',\n",
       " 'मेल्ट्डओओन',\n",
       " 'मेसेज',\n",
       " 'मोओड',\n",
       " 'मोटर',\n",
       " 'मोबइल',\n",
       " 'मोल',\n",
       " 'योओ.पेए.अ',\n",
       " 'योओँअर्स',\n",
       " 'योओक्रेन',\n",
       " 'योओजर्स',\n",
       " 'योओपेएअ',\n",
       " 'योओपेएएस',\n",
       " 'योओपेएसेएअ',\n",
       " 'योओरप',\n",
       " 'योओरोप',\n",
       " 'योओरोपेएयअ',\n",
       " 'रइक',\n",
       " 'रइलेएअ',\n",
       " 'रएएट',\n",
       " 'रडर',\n",
       " 'रन',\n",
       " 'रिँअअर्व',\n",
       " 'रिकअर्ड',\n",
       " 'रिकोर्ड',\n",
       " 'रिकोर्डिण्ग',\n",
       " 'रिक्टर',\n",
       " 'रिटयर्ड',\n",
       " 'रिण्ग',\n",
       " 'रिपोर्ट',\n",
       " 'रिलयण्स',\n",
       " 'रिलेएँअ',\n",
       " 'रिलेटड',\n",
       " 'रिलेस्हन्स्हप',\n",
       " 'रिव्योओअ',\n",
       " 'रेएएक्टर',\n",
       " 'रेएकउल',\n",
       " 'रेएपोर्ट',\n",
       " 'रेङरेएअ',\n",
       " 'रेट',\n",
       " 'रेडिएस्हन',\n",
       " 'रेडियोअ',\n",
       " 'रेमिठेन्स',\n",
       " 'रेल',\n",
       " 'रेल्गअरेएअ',\n",
       " 'रेल्गरेएअ',\n",
       " 'रेल्ब्हवन',\n",
       " 'रेल्वेअ',\n",
       " 'रेस्टरेन्ट',\n",
       " 'रोड',\n",
       " 'रोल',\n",
       " 'लअट्हेएच्हर्ज',\n",
       " 'लइण्स',\n",
       " 'लइन',\n",
       " 'लइन्डिन्ग',\n",
       " 'लइसेण्स',\n",
       " 'लउण्ढरिण्ग',\n",
       " 'लण्डन',\n",
       " 'लन्डन',\n",
       " 'लिण्क',\n",
       " 'लिस्ट',\n",
       " 'लेएक',\n",
       " 'लेएडर',\n",
       " 'लेएबेएयअअ',\n",
       " 'लेट',\n",
       " 'लेप्ह्ट',\n",
       " 'लोक्पअल',\n",
       " 'लोटरेएअ',\n",
       " 'वअठ्स',\n",
       " 'वअलेएअ',\n",
       " 'वउटर',\n",
       " 'वउलेठ',\n",
       " 'वर्मिण्ग',\n",
       " 'वर्ल्ड्कप',\n",
       " 'विकट',\n",
       " 'विकेएलेएक्स',\n",
       " 'विकेट',\n",
       " 'विग्यअन',\n",
       " 'विडियोअ',\n",
       " 'विपक्स्ह',\n",
       " 'विस्ह्वअ',\n",
       " 'वेए.सेए.अ',\n",
       " 'वेएँअअअ',\n",
       " 'वेएअएएपेएअ',\n",
       " 'वेएडियोअ',\n",
       " 'वेएप',\n",
       " 'वेएसेएअ',\n",
       " 'वेटर',\n",
       " 'वेटिकन',\n",
       " 'वेण्टिलेटर',\n",
       " 'वेण्टेएलेटर',\n",
       " 'वेब्सअइट',\n",
       " 'वेस्टिण्डेएस',\n",
       " 'वोट',\n",
       " 'वोन्टड',\n",
       " 'वोर्ल्ड्कप',\n",
       " 'वोल्टेज',\n",
       " 'व्यट्नअम',\n",
       " 'शेप्टेम्बेर',\n",
       " 'श्पेक्ट्रम',\n",
       " 'सअइकल',\n",
       " 'सअइट',\n",
       " 'सअइन',\n",
       " 'सअइन्सेस',\n",
       " 'सअइप्रस',\n",
       " 'सइण्टिमेएटर',\n",
       " 'सइण्ड्विच्ह',\n",
       " 'सइल्ङोन',\n",
       " 'सइल्सेएअस',\n",
       " 'सउङ्ट्वेयर',\n",
       " 'सटलइट',\n",
       " 'सब्सिडेएअ',\n",
       " 'सर्कट',\n",
       " 'सर्कोओलेस्हन',\n",
       " 'सर्टिङिचट',\n",
       " 'सर्विस',\n",
       " 'ससयठेएअ',\n",
       " 'सस्पइण्ड',\n",
       " 'सिग्रेट',\n",
       " 'सिटंबर',\n",
       " 'सिटंबर्.अ',\n",
       " 'सिटेएअ',\n",
       " 'सिण्डिकेट',\n",
       " 'सिनेमअअ',\n",
       " 'सिम',\n",
       " 'सिलेण्डर',\n",
       " 'सिस्टम',\n",
       " 'सुपर्मर्कट',\n",
       " 'सुपर्स्टर',\n",
       " 'सुप्रेएम',\n",
       " 'सेअ',\n",
       " 'सेए.एम्.अ',\n",
       " 'सेएट',\n",
       " 'सेएडेए.अ',\n",
       " 'सेएडेएअ',\n",
       " 'सेएपेएअएएअ',\n",
       " 'सेएपेएएम',\n",
       " 'सेएपेएसेएअ',\n",
       " 'सेएबेएअएएअ',\n",
       " 'सेएबेएसेएअ',\n",
       " 'सेएरियल',\n",
       " 'सेएरेएँअ',\n",
       " 'सेएल',\n",
       " 'सेएलिण्ग',\n",
       " 'सेन्टर',\n",
       " 'सेल्स',\n",
       " 'सोओनमेएअ',\n",
       " 'सोओप',\n",
       " 'सोडअवअटर',\n",
       " 'सोसअयटेएअ',\n",
       " 'सोस्हल',\n",
       " 'सोस्हलिस्ट',\n",
       " 'सोस्हलिस्म',\n",
       " 'स्कर्ट',\n",
       " 'स्केच्ह',\n",
       " 'स्कोओल',\n",
       " 'स्कोट',\n",
       " 'स्क्विड',\n",
       " 'स्टउर्म',\n",
       " 'स्टङ',\n",
       " 'स्टन्डिण्ग',\n",
       " 'स्टर्ट',\n",
       " 'स्टेएल',\n",
       " 'स्टेडेएअम',\n",
       " 'स्टेस्हन',\n",
       " 'स्टोरेएअ',\n",
       " 'स्ट्रोक',\n",
       " 'स्पेएकर',\n",
       " 'स्पेन',\n",
       " 'स्पेस्हल',\n",
       " 'स्मअर्ठ',\n",
       " 'स्य्स्टेम्)अ',\n",
       " 'स्विट्सेर्लइण्ड',\n",
       " 'स्विस',\n",
       " 'स्वेएडन',\n",
       " 'स्हटर',\n",
       " 'स्हर्क',\n",
       " 'स्हर्ट',\n",
       " 'स्हेएट',\n",
       " 'स्हेड्योओल',\n",
       " 'स्हेयर',\n",
       " 'स्होअ',\n",
       " 'स्होओटिन्ग',\n",
       " 'स्होक',\n",
       " 'हइक',\n",
       " 'हइकर',\n",
       " 'हइड्रोलोजिकल',\n",
       " 'हइण्ढ्लर',\n",
       " 'हइपोट्हेसिस',\n",
       " 'हउसिण्ग',\n",
       " 'हएएअ',\n",
       " 'हएएकोर्ट',\n",
       " 'हएएवय',\n",
       " 'हर्ट',\n",
       " 'हर्ड्वेयर',\n",
       " 'हिट',\n",
       " 'हेएरोइन',\n",
       " 'हेड',\n",
       " 'हेलोअ'}"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_engl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_engl.discard(\"अ\")\n",
    "data_engl.discard(\"अअ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"support_data/English_loanwords.txt\",\"r\") as g:\n",
    "    k=g.read().strip().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"support_data/eng_loan.txt\",\"r\") as g:\n",
    "    k=g.read().strip().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_loanwords = set(k).union(data_engl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_loanwords = set(k).union(english_loanwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1099"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_loanwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0]"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputString = [\"dk\",\"7\",\"3\",\"fd\",\"fg\"]\n",
    "[1 if char.isdigit() else 0 for char in inputString]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
